# Integrated stock assessment models {#integratedassessmentmodels}



## Process model {#ProcessIntegrated}



### Fishing mortality {-}
A quick reminder of the commonly applied fishing dynamics applied in stock assessment models. A good general overview is by @branch2009differences. They compare the continuous Baranov Catch equation [@baranov1918question] vs Pope's discrete formulation [@pope1972investigation]. Arguments for using the continuous case is that $M$ and $F$ occur simultaneously, also with the continuous case, $F$ allows for multiple event encounters, this is assuming a fleet has the same selectivity and availability, that a fish that escapes one net can be caught in another. In contrast, the discrete formulation only allows a fish to be caught or escape from an instantaneous event. I have summarized the benefits of the continuous equation in the following list,

*  Allows the entire population to be caught (not sure this is that relevant)
*  Allows simultaneous $M$ and $F$, no need to worry about order of operations. From a coding/practical perspective this is quite attractive. Once you have an F and M you can easily derive all mid-mortality quantities. Where as using the \(U\) approach you need save the population before and after to interpolate to derive mid-mortality quantities.
*  $F$ effects Composition data, where as in the discrete case composition is independent of $U$.
*  Allows for multiple catch events of an individual
*  Can fit to catch thus allows for uncertainty in catches

The arguments for the discrete approximation is that there is an analytical solution for \(U\) and so is fast to calculate expected Catch, where as $F$ has to be either solved numerically or estimated as a free parameter. 
 
Chris Francis's wrote a response to this paper [@francisCommentBranch] where he argues the discrete formulation does not preclude the multiple encounters and that only the data can truly tell us which catch equation is the best one to use.
 
The relationship between $F$ (Instantaneous fishing mortality) and $U$ exploitation rate for a simple scenario (single fishery) is given in the following R code.

```{r illustrate_F_vs_U_sim}
## Params
ages = 1:20
R0 = 1000
M = 0.2
a50 = 3.4
ato95 = 2.4
A = length(ages)
N_age = vector()
N_age[1] = R0
set.seed(123)
for(age_ndx in 2:A)
  N_age[age_ndx] = N_age[age_ndx - 1] * exp(-M) * exp(rnorm(1,0,0.5))

barplot(height = N_age, names = ages)

# selectivity at age
S_age = stockassessmenthelper:::logis_sel(ages, a50, ato95)

## No selectivity
u_range = seq(0,0.8,by = 0.02)
F_range = -log(1 - u_range)
# 1 - exp(-F_range) # back calculate U
```



```{r illustrate_F_vs_U, fig.fullwidth=T}
## the application throught time.
N_1 = 100
par(mfrow = c(2,2), mar = c(2,2,2,1), oma = c(3,2,2,0))
Fs = c(0.2,0.6,1,1.4)
for(i in 1:length(Fs)) {
  F_t = Fs[i]
  U_t = 1 - exp(-F_t)
  M = 0.5
  time_ = seq(0,1, by = 0.001)
  change_over_time = N_1 * exp(-(F_t+M)*time_)
  change_over_time_alt = N_1 * exp(-M*time_[time_<0.5]) 
  change_over_time_alt = c(change_over_time_alt, change_over_time_alt[length(change_over_time_alt)] * (1 - U_t))
  change_over_time_alt = c(change_over_time_alt, change_over_time_alt[length(change_over_time_alt)] * exp(-M*time_[time_ < 0.5]) )
  
  plot(1,1, type = "n", xlab = "", ylab = "", ylim = c(0,100), xlim = c(0,1), xaxt = "n", yaxt = "n", cex.main = 1.5,cex.lab = 1.5, main = substitute(paste(F[y], " = ", this_F, " M = ", M), list(this_F = F_t, M= M)))#paste0(, " = ", F_t))
  lines(time_, change_over_time, lwd = 4)
  lines(time_, change_over_time_alt, lwd = 4, col = "red")
  if (i > 2)
    axis(side = 1, tick = T, at = c(0,1), labels = c("y", "y+1"), cex.axis = 2)
  if (i == 1)
    legend('bottomleft', legend = c("Baranov","Discrete"), lwd = 3, col = c("black","red"), cex = 0.8)
}
mtext(side = 1, text = "Time", outer = T, line = 0.7, cex = 1.3)
mtext(side = 2, text = "N", outer = T, line = -1, cex = 1.3)
```
## Observation model {#observationIntegrated}




### Data weighting {-}
Data-weighting is the procedure of allowing for better representation of uncertainty to compensate for model misspecification. This results in better capturing the total error which is a combination of process error (model misspecification and process variation i.e., time-varying dynamics) and observation error (sampling error)



This section reviews the literature and describes current practices and beliefs when dealing with model-misspecification in an integrative analysis, which is often termed "data-weighting". In integrative statistical population models the weight assigned to each data set and within data sets controlled by the variance parameter of the likelihood function has been shown to have significant consequences on estimated parameters and derived quantities [@maunder2017data;@francis2011data].




Data conflict occurs when two or more data-sets contain different information on process states or parameters conditional on a given model structure [@maunder2017dealing]. How modellers have dealt with this situation when it arises has been varied [@francis2011data] and is an active area of research in the stock assessment literature [@maunder2017data]. There are a few methods and procedures to deal with "data-conflict" that we will discuss and offer some methods for moving forward to a unified frame-work.



A common suggestion or conversation in technical working groups when this situation arises is to assume one of the data-sources are un-representative of the system and so decrease the assumed precision of the likelihood contribution or remove the data-set all together. This is an ad-hoc method and as suggested by many, the decision of whether a data-set is representative should be made at the model inputs stage not after the fitting procedure. As once we have confidence in a data-set to be considered as an input for the stock assessment it should be considered "fact" @maunder2017dealing.



There have also been procedure recommended by @francis2011data which suggested the following steps,

* do not let other data stop the model from fitting abundance data well.
* when weighting age or length composition data, allow for correlations.
* do not down-weight abundance data because they may be unrepresentative.

The reasoning behind these recommendations revolve around the importance of abundance signals in the context of the objectives of stock assessments, being mostly related to abundance. The second point is still important when observing residuals but with the recent inclusion of likelihoods that can allow for correlation among age and length. The third point is re-iterating the early methods of being cautious when dealing with un-representative data.



The most recent advancement is the use of proper (as apposed to improper likelihoods such as the multinomial) observational likelihoods for compositional data, which have variance parameters that can be estimated by the model, such as the dirichlet and Dirichlet multinomial [@thorson2017model]. This approach has mainly been used in the state-space model framework, where comments are often made for example from @nielsen2014estimation "The state-space formulation of the model has several additional benefits which include: estimation of observation error which allows for objective weighting of multiple data sources," and from @cadigan2015state "This is done using an approach that avoids the use of subjective data-weighting". The benefit of estimating the weighting parameters in the assessment framework is the propagation of uncertainty of this some times influential parameter.



@francis2017revisiting argued that although "when we add process variation to a model we are likely to reduce the process error associated with an observation (and thus allow greater weight to be applied to the observation). However, it would be wrong to assume that we can remove all process error in a state-space stock assessment model because not all process error is associated with process variation: some is due to other factors, such as errors in either fixed parameters (e.g.,using the wrong parameter values for natural mortality or growth) or mathematical forms"



There has been discrepancies I feel in state-space models in the literature where both process error and observation error have been estimated. This seem to be with confounding and parameter identifiability of the variance terms of both the process component and the observational component. This is the area of interest that I wish to pursue during my PhD. The specific questions and hypothesis that I have in mind are.


* What data is needed to allow for confident estimation of process error and estimation error
	

	
*	How can we extend the data weighting concept into state-space model framework where we can allow for the model to represent total error as Chris puts it	
	
* How to identify model mis-specification when using frequencies?
	
* What are the consequences of getting the process error terms wrong on model outputs (when multiple states are models)?
	
* are there merits in some state-space model formulations over others.


These are general questions and I actually don't have any hypothesis for them so I am going to generate a simulation study to investigate these questions from a naive view point that I am not sure a if they are even answerable. So the emphasis will be on methodology so that I have a robust 'experiment' so that these questions can be investigated.


