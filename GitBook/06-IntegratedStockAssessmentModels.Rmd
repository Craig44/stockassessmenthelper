# Integrated stock assessment models {#integratedassessmentmodels}



## Process model {#ProcessIntegrated}



### Fishing mortality {-}
A quick reminder of the commonly applied fishing dynamics applied in stock assessment models. A good general overview is by @branch2009differences. They compare the continuous Baranov Catch equation [@baranov1918question] vs Pope's discrete formulation [@pope1972investigation]. Arguments for using the continuous case is that $M$ and $F$ occur simultaneously, also with the continuous case, $F$ allows for multiple event encounters, this is assuming a fleet has the same selectivity and availability, that a fish that escapes one net can be caught in another. In contrast, the discrete formulation only allows a fish to be caught or escape from an instantaneous event. I have summarized the benefits of the continuous equation in the following list,

*  Allows the entire population to be caught (not sure this is that relevant)
*  Allows simultaneous $M$ and $F$, no need to worry about order of operations. From a coding/practical perspective this is quite attractive. Once you have an F and M you can easily derive all mid-mortality quantities. Where as using the \(U\) approach you need save the population before and after to interpolate to derive mid-mortality quantities.
*  $F$ effects Composition data, where as in the discrete case composition is independent of $U$.
*  Allows for multiple catch events of an individual
*  Can fit to catch thus allows for uncertainty in catches

The arguments for the discrete approximation is that there is an analytical solution for \(U\) and so is fast to calculate expected Catch, where as $F$ has to be either solved numerically or estimated as a free parameter. 
 
Chris Francis's wrote a response to this paper [@francisCommentBranch] where he argues the discrete formulation does not preclude the multiple encounters and that only the data can truly tell us which catch equation is the best one to use.
 
The relationship between $F$ (Instantaneous fishing mortality) and $U$ exploitation rate for a simple scenario (single fishery) is given in the following R code.

```{r illustrate_F_vs_U_sim}
## Params
ages = 1:20
R0 = 1000
M = 0.2
a50 = 3.4
ato95 = 2.4
A = length(ages)
N_age = vector()
N_age[1] = R0
set.seed(123)
for(age_ndx in 2:A)
  N_age[age_ndx] = N_age[age_ndx - 1] * exp(-M) * exp(rnorm(1,0,0.5))

barplot(height = N_age, names = ages)

# selectivity at age
S_age = stockassessmenthelper:::logis_sel(ages, a50, ato95)

## No selectivity
u_range = seq(0,0.8,by = 0.02)
F_range = -log(1 - u_range)
# 1 - exp(-F_range) # back calculate U
```



```{r illustrate_F_vs_U, fig.fullwidth=T}
## the application throught time.
N_1 = 100
par(mfrow = c(2,2), mar = c(2,2,2,1), oma = c(3,2,2,0))
Fs = c(0.2,0.6,1,1.4)
for(i in 1:length(Fs)) {
  F_t = Fs[i]
  U_t = 1 - exp(-F_t)
  M = 0.5
  time_ = seq(0,1, by = 0.001)
  change_over_time = N_1 * exp(-(F_t+M)*time_)
  change_over_time_alt = N_1 * exp(-M*time_[time_<0.5]) 
  change_over_time_alt = c(change_over_time_alt, change_over_time_alt[length(change_over_time_alt)] * (1 - U_t))
  change_over_time_alt = c(change_over_time_alt, change_over_time_alt[length(change_over_time_alt)] * exp(-M*time_[time_ < 0.5]) )
  
  plot(1,1, type = "n", xlab = "", ylab = "", ylim = c(0,100), xlim = c(0,1), xaxt = "n", yaxt = "n", cex.main = 1.5,cex.lab = 1.5, main = substitute(paste(F[y], " = ", this_F, " M = ", M), list(this_F = F_t, M= M)))#paste0(, " = ", F_t))
  lines(time_, change_over_time, lwd = 4)
  lines(time_, change_over_time_alt, lwd = 4, col = "red")
  if (i > 2)
    axis(side = 1, tick = T, at = c(0,1), labels = c("y", "y+1"), cex.axis = 2)
  if (i == 1)
    legend('bottomleft', legend = c("Baranov","Discrete"), lwd = 3, col = c("black","red"), cex = 0.8)
}
mtext(side = 1, text = "Time", outer = T, line = 0.7, cex = 1.3)
mtext(side = 2, text = "N", outer = T, line = -1, cex = 1.3)
```
## Observation model {#observationIntegrated}




### Data weighting {-}
Data-weighting is the procedure of allowing for better representation of uncertainty to compensate for model misspecification. This results in better capturing the total error which is a combination of process error (model misspecification and process variation i.e., time-varying dynamics) and observation error (sampling error)



This section reviews the literature and describes current practices and beliefs when dealing with model-misspecification in an integrative analysis, which is often termed "data-weighting". Data weighting is an important consideration when providing management advice. In stock assessments, weighting of a data set is conducted by adjusting the variance parameter of the corresponding likelihood function (e.g. the $\sigma$ for the lognormal and $N$ for multinomial), and has been shown to have significant impacts on management advice [@maunder2017data;@francis2011data] (Figure \@ref(fig:b0conflict)). Data weighting is conducted to address data conflict and data prioritization. 



Data conflict is the phenomenon that occurs when multiple data sets, conditioned on a model structure, provide information about a model parameter or estimated quantity that disagree. Given models are simplifications of reality and data are assumed facts the conflict must arise from model misspecification, but interpretation must account for random sampling error [@maunder2017dealing]. Figure \@ref(fig:b0conflict) demonstrates an example of data conflict for a stock assessment between age composition data and abundance data. How modellers deal with data conflict has been varied [@francis2011data] and is an active area of research in the stock assessment literature [@maunder2017data]


Data prioritisation is the consideration of whether there are *a priori* arguments for one data set to have more influence over another. There have been arguments made in the stock assessment literature that independent data should have more weight (be more influential) [@cotter2004stock]. There have also been strong advocating that abundance or biomass data types should take precedence because management outcomes are directly related to these quantities [@rancis2011data].




![(#fig:b0conflict)A likelihood profile on unfished biomass parameter, $B_0$, illustrating data conflict, between abundance and age composition data. Each plotted line shows how the objective function (i.e., the negative log-likelihood) associated with some subset of the data varies across the profile, with each line being zero adjusted (i.e., shifted vertically so that its minimum (marked  "X") occurs at 0). The profile is from the 2010 assessment of New Zealand hake [@horn2010stock].](Figures/dat_weighting.png)


A common suggestion or conversation in technical working groups (from my experience) when this situation arises is to assume one of the data-sources are un-representative of the system and so decrease the precision of the likelihood contribution or remove the data-set all together. This is an ad-hoc method and as suggested by many, the decision of whether a data-set is representative should be made at the model inputs stage not after the fitting procedure. Once we have confidence in a data-set to be considered as an input for the stock assessment it should be considered "fact" @maunder2017dealing.


There have also been procedure recommended by @francis2011data which suggested the following steps,

* do not let other data stop the model from fitting abundance data well.
* when weighting age or length composition data, allow for correlations.
* do not down-weight abundance data because they may be unrepresentative.

The reasoning behind these recommendations revolve around the importance of abundance signals in the context of the objectives of stock assessments, being mostly related to abundance being the key stock status quantity. The second point is important when observing residuals but with the recent inclusion of likelihoods that can allow for correlation among age and length it should be becoming more routine. The third point is re-iterating the early comments about heed caution when dealing with un-representative data.


There are three methods used for data weighting in stock assessments: *outside the model*, where weights are calculated and fixed during model estimation, *inside the model* where appropriate likelihoods are applied and likelihood variance parameters estimated, and *iterative* weighting [@francis2017revisiting], which applies the following algorithm.
* Set initial weights for the data set
* Conduct model estimation
* Use information from the model output to adjust the data weights
* Repeat steps 2 and 3 as often as desired


While it has been used, the *outside the model* weighting method is not recommended [@francis2017revisiting]. The most objective method is the *inside the model* weighting method, choosing likelihoods that have variance parameters that can be estimated (self-weighting) for all data sets e.g. Dirichlet multinomial [@thorson2017model] [@aitchison1982statistical; @francis2014replacing] for compositional data and normal and lognormal for abundance time-series. Self-weighting likelihoods are becoming more prominent in stock assessment models but are still not the norm [@francis2017revisiting].


The *iterative* weighting method is the most commonly applied method in New Zealand stock assessment models [@plenary_18]. Where for an observation denoted $O_t$ and a value expected by the model denoted $E_t$ at time $t$. The *Iterative* weighting methods aims to standardise the error $(O_t - E_t)$ so that the standardised errors ($S_{t}$) have constant variance for all time steps (and if age composition data for all ages) i.e. $S_{t} = (O_{t} - E_{t}) /N_{t}$, where $N_{t}$ is a function of the weighting parameter that can be updated during the iterative algorithm [@francis2011data]. The *iterative* weighting method is considered subjective for several reasons. The first reason is the choice of algorithms. There are a range of algorithms in the literature but there is no "best" algorithm [@punt2017some]. Secondly, data prioritisation has been advocated for with respect to abundance: for instance, @francis2011data recommends "do not let other data stop the model from fitting abundance data well". An interest of mine is looking at methods that remove subjectivity from stock assessments.



The most recent advancement is the use of proper (as apposed to improper likelihoods such as the multinomial) observational likelihoods for compositional data, which have variance parameters that can be estimated as a parameter in the model, such as the dirichlet and Dirichlet multinomial [@thorson2017model]. This approach has mainly been used in the state-space model framework, where comments are often made for example from @nielsen2014estimation "The state-space formulation of the model has several additional benefits which include: estimation of observation error which allows for objective weighting of multiple data sources," and from @cadigan2015state "This is done using an approach that avoids the use of subjective data-weighting". The benefit of estimating the weighting parameters within the model is the propagation of uncertainty through to derived quantities.



@francis2017revisiting argued that "when we add process variation to a model we are likely to reduce the process error associated with an observation (and thus allow greater weight to be applied to the observation). However, it would be wrong to assume that we can remove all process error in a state-space stock assessment model because not all process error is associated with process variation: some is due to other factors, such as errors in either fixed parameters (e.g.,using the wrong parameter values for natural mortality or growth) or mathematical forms"






